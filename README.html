<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>README.md - Grip</title>
  <link rel="icon" href="data:image/x-icon;base64,AAABAAEAEBAAAAEACABoBQAAFgAAACgAAAAQAAAAIAAAAAEACAAAAAAAAAIAAAAAAAAAAAAAAAAAAAAAAAAKCgr/FRUX/xYWGP8YGBr/GRkb/xsbHP8dHR7/Hh4f/x4eIP8fHyD/ICAh/yMjJP8kJCX/JiYo/ygoKv8pKSr/LCwu/y0tLv80NDT/QUFC/0ZGR/9MTE7/UFBR/1JSUv9bW1v/XFxc/2BgYP9hYWP/aGhp/2xsbP9ycnL/d3d3/3l5e/98fHz/f3+A/2dn/P9z3a//cNX8/4CAgP+Xl5f/nZ2e/56en/+fn5//n5+h/6amp/+rq6v/rKys/7CvsP+zs7T/uLi5/7u7u/+8vLz/wMDA/8fHx//Kysr/zc3N/9HR0f/d3d3/3t7e/+Xl5f/q6ur/6+vr/+3t7f/z8/P/9fX1//b29v/39/f/+Pj4//n5+f/6+vr/+/v7//z8/P/9/f3//v7+//////8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/NCYmJiYmJiYmJiYmJiYmNCZKSkpKSkpKSkpKSkpKSiYmSkpKRisdRkMbMklKSkomJkpKSSEiNUhEEQwpSUpKJiZKSi8LHhhAPAwBDDZKSiYmSkoWACA7SEc6HAEaSkomJkpKBxI/SkpKSj4ICkpKJiZKSgUTR0pKSkpGDwZKSiYmSkoTED1KSkpKOQQVSkomJkpKKgowMy4sNycIMUpKJiZKSkEZDgkDAgsNH0VKSiYmSkpKQigUBgkXLUhKSkomJkpKSkpKSkpKSkpKSkpKJiYmJiYmJiYmJiYmJiYmJiYmIyUkNDQ0NDQ0NDQ0NDQmOCYmJiYmJiYmJiYmJiYmOAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=" />
  <link rel="stylesheet" href="//octicons.github.com/components/octicons/octicons/octicons.css" />
  <style>
    /* Page tweaks */
    .preview-page {
      margin-top: 64px;
    }
    /* User-content tweaks */
    .timeline-comment-wrapper > .timeline-comment:after,
    .timeline-comment-wrapper > .timeline-comment:before {
      content: none;
    }
    /* User-content overrides */
    .discussion-timeline.wide {
      width: 920px;
    }
  </style>
</head>
<body>
  <div class="page">
    <div id="preview-page" class="preview-page" data-autorefresh-url="">



      <div role="main" class="main-content">
        <div class="container new-discussion-timeline experiment-repo-nav">
          <div class="repository-content">
            <div id="readme" class="readme boxed-group clearfix announce instapaper_body md">

                <h3>
                  <span class="octicon octicon-book"></span>
                  README.md - Grip
                </h3>

              <article class="markdown-body entry-content" itemprop="text" id="grip-content">
                <h1 align="center">
<a id="user-content-neuralgrewtneural-grammar-rule-extraction-and-word-taxonomy" class="anchor" href="#neuralgrewtneural-grammar-rule-extraction-and-word-taxonomy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>NeuralGREWT:<br>Neural Grammar Rule Extraction and Word Taxonomy</h1>
<p><a href="http://blackboxlabs.dev/neuralgrewt" rel="nofollow"><img src="repo_images/plot.png" alt="visualization" title="Interact" style="max-width:100%;"></a>
<a href="http://blackboxlabs.dev/neuralgrewt/?fig=ng_pos_15.0" rel="nofollow"><p align="center">Click to Interact</p></a></p>
<p>Unsupervised learning and comprehensible representation of grammar rules and
fuzzy symbol categories by high-dimensional clustering of string probabilities
derived from natural language models and Comprehensible Convolutional Neural
Networks (CCNN).</p>
<h2>
<a id="user-content-theory" class="anchor" href="#theory" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Theory</h2>
<p>A <em>grammar</em> is a set of rules for how symbols<sup id="user-content-a1"><a href="#f1">1</a></sup>
may be arranged to form valid strings in a language. Strings of symbols  that
follow all grammar rules are grammatically <em>valid</em>.  Grammatic validity
is necessary for, but does not imply, sensicality.  For example: the string of
symbols (in this case a sentence of words) "The hat runs and cooks into a
hairdresser." is a grammatically valid sentence that is also nonsensical in
isolation<sup id="user-content-a2"><a href="#f2">2</a></sup>.</p>
<p>Given this definition of grammatic validity, valid sentences will be more
common than invalid ones in any string-set comprised mostly of sensical
strings; making it possible to infer a string's validity from its probability.</p>
<p>Therefore, it may be possible to learn grammar rules of any language, including
computer, fictional, and extra-terrestrial languages, without needing to make
sense of anything written in the given language<sup id="user-content-a3"><a href="#f3">3</a></sup>.</p>
<p>Let's test this hypothesis.</p>
<p>One good first step to learning grammar rules may be to identify the
different categories of symbols present in a given language.</p>
<p>Symbols will be said to share a syntactic <em>category</em> in proportion to
their mutual interchangability.  In other words: 2 symbols will share a
category in proportion to the probability that one may be replaced by the other
in a randomly chosen valid string without rendering that string invalid.</p>
<p>Knowing this, a natural language generator<sup id="user-content-a4"><a href="#f4">4</a></sup> can be
used to determine the degree of mutual interchangabilty of symbols in a
language using total string likelihood before and after replacement as an
indicator of relative validity, and therefore of mutual interchangeability
â€” implying taxonomy.  Once we have these validity scores we can use T-SNE
to infer discrete symbol categories from the degree of mutual interchangability
(or equivalently the clustering of validity-under-replacement scores).</p>
<h2>
<a id="user-content-procedure" class="anchor" href="#procedure" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Procedure</h2>
<ul>
<li>Compile a large <em>corpus</em> of valid and/or sensical writings in the
chosen language.</li>
<li>Train a natural language model, or <em>predictor</em>, on the corpus until it
is very good at predicting
symbols in the language given context. Anything like BERT or GPT-2 will do
fine, and in fact are probably overkill.
<ul>
<li>NOTE: It's definitely overkill. Future work will employ a
maximally-minimal version of BERT to accelerate calculation of validity
tensors and eliminate "understanding intrusion". Large BERT and GPT-2 models
possess a good deal of understanding of written text that leads them to rank
the probability of valid but otherwise non-sensical strings signficantly
lower than they would if they understood only base grammar
relationships. Clearly this is not good if we are looking for a measure of
base validity regardless of sensicality<sup id="user-content-a5"><a href="#f5">5</a></sup>.</li>
</ul>
</li>
<li>Compile a large "ground set" of valid and/or meaningful strings in the chosen language.
(in the case of unknown languages, this ground set can just be a random
subset of the corpus)</li>
<li>Compile a "symbol set" of all symbols in the chosen language, or at least a large
set of the most common ones.</li>
<li>Create a 4 dimensional <em>perturbation tensor</em> from the ground and symbol sets by:
<ul>
<li>Add to the perturbation tensor a <em>cube</em> for each <em>symbol</em>
in the symbol set; construct each cube by stacking a <em>plane</em> for
each <em>ground string</em> in the ground set; construct each plane by stacking
a <em>vector</em> for all strings that can be created by replacing any one item
in the ground string with the symbol. Of course, making sure to do this in the
same order for all vectors<sup id="user-content-a6"><a href="#f6">6</a></sup>.</li>
</ul>
</li>
<li>Create a 4 dimensional <em>validity tensor</em> from the perturbation tensor by:
<ul>
<li>For each vector in the perturbation tensor, judge the probability
of that vector by summing the relative likelihoods of each symbol
appearing at its location given all previous symbols in the vector (using the
predictor), taking the difference between this and the sum-validity of it's
corresponding ground string to obtain a <em>validity delta</em>, and dividing
by the length of that vector. That division might be unnecessary, but we will
find out.
<ul>
<li>Update: It occurs to me that the division by sentence length would
have a largely unhelpful effect on the validity score of any one
dimension relative to all the others; exaggerating similarities between
dimensions as vectors grow longer. The ground-truth validity delta may or may
not have anything to do with string length depending on the language so it is
wrong to force such a correlation in general.  A proper normalization across
all dimensions regarded equally is what we want here.</li>
</ul>
</li>
</ul>
</li>
<li>Perform T-SNE followed by PCA on the validity tensor to infer number and
relative importance of symbol categories.
<ul>
<li>NOTE: In this case PCA will not, and cannot provide a useful classifier
for datapoints that were not already present in the T-SNE plot.  Here we
are simply looking to quantify the number of clusters, and get some small idea
of the distances between clusters<sup id="user-content-a7"><a href="#f7">7</a></sup>.</li>
</ul>
</li>
<li>Name each symbol category. (can be totally arbitrary)</li>
<li>Create a <em>sym-cat</em> mapping of each symbol to a list of its categories
sorted in descending order of the symbol's <em>belongingness</em> to each category.</li>
<li>Create a <em>cat-sym</em> mapping of each category to a list of its symbols
sorted in descending order of each symbol's belonging-ness to the category.</li>
<li>Create a <em>token set</em> from the corpus by replacing each symbol in the
corpus with a name, or <em>token</em>, for its corresponding category<sup id="user-content-a8"><a href="#f8">8</a></sup>.</li>
<li>Create a <em>garbage set</em> from the token set by randomizing a large
enough proportion of the tokens in the token set to render each string likely
invalid.</li>
<li>Train a comprehensible, spiking, convolutional neural network, or
<em>grammar net</em>, to distinguish between items of the token and garbage
sets.
<ul>
<li>The filters of the trained grammar net are first order grammar rules, the
fully connected layers behind those are second order grammar rules, and
so on. (or something to that effect depending on the structure of the grammar
net.)</li>
</ul>
</li>
</ul>
<h2>
<a id="user-content-status" class="anchor" href="#status" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Status</h2>
<ul>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" checked="" disabled=""> Gather Corpus</li>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" checked="" disabled=""> Train Natural Language Model (GPT-2)</li>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" checked="" disabled=""> Compile Ground Set</li>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" checked="" disabled=""> Compile Symbol Set</li>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" checked="" disabled=""> Generate Perterbation Tensor</li>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" checked="" disabled=""> Calculate Validity Tensor
<ul>
<li>Signs from the first few checkpoints are looking very promising.
Replacements known to largely preserve validity (such as pronouns with
any other pronoun) are displaying markedly higher probability to the predictor.
The trend is so striking that it is visible to the naked eye even before
normalization. Feels like cheating a little bit, since I am trying to stick to
a pre-registered method, and peeking at the data while it's being processed
can't be good for my objectivity. I did have to look at least once to make sure
that the program was calculating what I meant it to, so I suppose it can't be
helped that I went looking for patterns like these. I'll just have to be extra
careful not to let this good news inform any of my future decisions. Best way
to do that is to make as few of them as possible and stick closely to the plan
as written.</li>
<li>[ Trial set generated in 10 days of computation time on a GTX 1070 M ]</li>
</ul>
</li>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" checked="" disabled=""> T-SNE
<ul>
<li>To improve our dynamic range, each dimension of the validity tensor
is scaled independently of the others so that its values now
range between -1 and 1 for all symbols. This is to ensure signals of
dimensions with naturally smaller values are not so severly washed-out by
signals of naturally larger dimensions in the next step.</li>
<li>After scaling, the validity tensor is passed through a heaviside function to produce
a tensor of sparse boolean vectors with ones for all values above a
specified threshold.  Implicitly, this means that all values above some
high-but-ultimately-arbitrary validity threshold<sup id="user-content-a9"><a href="#f9">9</a></sup> are
regarded as totally valid replacements, and all others are assumed to be
equally invalid.</li>
<li>T-SNE was performed using the jaccardian dissimilarity index of each
vector against every other vector as a distance metric together with
Barnes-Hut approximation to find a good representation of the clustering in a
visualizable space.</li>
<li>The step above was performed with many perplexities ranging from 5 to 50.
Qualitative optimum was found to lie between 12.5 and 17.5<sup id="user-content-a10"><a href="#f10">10</a></sup>.</li>
</ul>
</li>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" checked="" disabled=""> <del>PCA</del>
<ul>
<li>It is clear from the visualization that PCA is not the best choice here
afterall. The trouble now is not in finding those dimensions that best
define the data, but in finding a way to idetify the boundaries of distinct
(and not-so-distinct) t-sne clusters in the visualizable space with minimal if
any human involvement. DBSCAN seems well suited to this task. Barring that,
many of these clusters seem isolated enough to eyeball without injecting too
much subjectivity (hence the reduction to visualizable space), but an automatic
approach is always preferable.</li>
</ul>
</li>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled=""> Automatic Cluster Identification with DBSCAN</li>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled=""> Category Naming and Mapping</li>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled=""> Generate Token and Garbage Sets</li>
<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled=""> Train Grammar Net
<ul>
<li>One good way to make the grammar net more comprehensible would be to use
a very large number of filters in the convolutional layers and
<strong>strong</strong> regularization across the board, so as to ensure that
the vast majority of filters obviously do nothing<sup id="user-content-a11"><a href="#f11">11</a></sup>, while emphasizing the important few that communicate
a lot.</li>
</ul>
</li>
</ul>
<h2>
<a id="user-content-results" class="anchor" href="#results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results</h2>
<ul>
<li>Word Taxonomy Visualization:</li>
</ul>



<div>
<p align="center">
<a href="http://blackboxlabs.dev/neuralgrewt/?fig=ng_pos_15.0" rel="nofollow">
<img src="./repo_images/smallPlot.png" alt="scatter plot" width="500" height="550" align="center" style="max-width:100%;">
</a>
</p>
<a href="http://blackboxlabs.dev/neuralgrewt/?fig=ng_pos_15.0" rel="nofollow">
<p align="center">Click to Interact in 3D</p>
</a>
<br>
<br>
<p align="center">
<a href="./repo_images/legend.svg" target="_blank" rel="noopener noreferrer"><img src="./repo_images/legend.svg" alt="legend" align="center" style="max-width:100%;"></a>
</p>
</div>
<p>[TBD]</p>
<h2>
<a id="user-content-footnotes" class="anchor" href="#footnotes" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Footnotes</h2>
<p><b id="user-content-f1">1</b> More precisely: "morphemes" composed of symbols. <a href="#a1"><g-emoji class="g-emoji" alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">â†©</g-emoji></a></p>
<p><b id="user-content-f2">2</b> "in isolation" meaning in the absence of explanatory context. <a href="#a2"><g-emoji class="g-emoji" alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">â†©</g-emoji></a></p>
<p><b id="user-content-f3">3</b> So long as we are in possession of an ample body of text that
would be sensical to a speaker of that language. <a href="#a3"><g-emoji class="g-emoji" alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">â†©</g-emoji></a></p>
<p><b id="user-content-f4">4</b> Any predictor of morphemes from context really. <a href="#a4"><g-emoji class="g-emoji" alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">â†©</g-emoji></a></p>
<p><b id="user-content-f5">5</b> This will also ensure that we get a good measurement of
validity in some imaginable language where it is impossible to have
grammatically valid strings that don't make sense. This would mean only that
validity implies sensicality. In such languages, learning base validity is
identical to learning sensicality, which may-or-may-not require a larger
network to accomplish (interesting philosophical question there) but is
nevertheless possible as demonstrated by large language models of recent years.
<a href="#a5"><g-emoji class="g-emoji" alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">â†©</g-emoji></a></p>
<p><b id="user-content-f6">6</b> In an actual tensor all such vectors need to be padded to
uniform dimension with null strings. In practice we can use a 3 dimensional
tensor of pointers to vectors of variable dimension. <a href="#a6"><g-emoji class="g-emoji" alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">â†©</g-emoji></a></p>
<p><b id="user-content-f7">7</b> Distances between T-SNE clusters are sometimes meaningless, so
the distances according to PCA are not to be taken seriously in this case.
However, the number of clusters and degree of overlap communicated by PCA on
T-SNE will be reliably meaningful if good clusters are found.</p>
<ul>
<li>NOTE: this subtlety is obviated by PCA's replacement with DBSCAN in the
cluster identification step. <a href="#a7"><g-emoji class="g-emoji" alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">â†©</g-emoji></a>
</li>
</ul>
<p><b id="user-content-f8">8</b> In the proof of concept we will simply pick the highest ranked
category from each symbol's entry in the sym-cat mapping. Future work will be
needed to make an informed choice of category in the case of context dependent
taxonomy. <a href="#a8"><g-emoji class="g-emoji" alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">â†©</g-emoji></a></p>
<p><b id="user-content-f9">9</b> In this case above -5 or thereabouts after scaling. <a href="#a9"><g-emoji class="g-emoji" alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">â†©</g-emoji></a></p>
<p><b id="user-content-f10">10</b> Specifically: optimimum for a set of 500 points in ~5500
dimensions. This optimimum is likely to change depending on your point-count to dimensionality ratio. <a href="#a10"><g-emoji class="g-emoji" alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">â†©</g-emoji></a></p>
<p><b id="user-content-f11">11</b> or are lower-weighted, redundant duplicates that all do the
same thing <a href="#a11"><g-emoji class="g-emoji" alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">â†©</g-emoji></a></p>
<h2>
<a id="user-content-how-to-install" class="anchor" href="#how-to-install" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How to Install</h2>

<p>[TBD]</p>
<h2>
<a id="user-content-how-to-use" class="anchor" href="#how-to-use" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How to Use</h2>
<p>[TBD]</p>
<h2>
<a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>
<p>For contributors to the project; do this before making your first commit:</p>
<ul>
<li>Install pre-commit</li>
</ul>
<div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span> /path/to/this/repository/
sudo apt install pre-commit
pre-commit install</pre></div>
<p>(we do all of our development on linux)</p>
<ul>
<li>To test updates to the readme and other GitHub flavored markdown, simply install Grip
and feed it your desired file.</li>
</ul>
<div class="highlight highlight-source-shell"><pre>pip3 install grip
python3 -m grip README.md</pre></div>
<ul>
<li>
<p>Then follow the link provided by the Grip sever for a live preview of your work.</p>
</li>
<li>
<p>When satisfied with your changes you can compile to an html file with:</p>
</li>
</ul>
<div class="highlight highlight-source-shell"><pre>python3 -m grip README.md --export README.html</pre></div>
<h2>
<a id="user-content-authors" class="anchor" href="#authors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors</h2>
<ul>
<li>
<strong>Gabe M. LaFond</strong> - <em>Initial work</em> - <a href="https://github.com/ExamDay">ExamDay</a>
</li>
</ul>
<p>See also the list of <a href="https://github.com/ExamDay/NeuralGREWT/contributors">contributors</a> who participated in this project.</p>
<h2>
<a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>
<p>This project is licensed under the MIT License - see the <a href="LICENSE.md">LICENSE.md</a> file for details</p>

              </article>
            </div>
          </div>
        </div>
      </div>



  </div>
  <div>&nbsp;</div>
  </div><script>
    function showCanonicalImages() {
      var images = document.getElementsByTagName('img');
      if (!images) {
        return;
      }
      for (var index = 0; index < images.length; index++) {
        var image = images[index];
        if (image.getAttribute('data-canonical-src') && image.src !== image.getAttribute('data-canonical-src')) {
          image.src = image.getAttribute('data-canonical-src');
        }
      }
    }

    function scrollToHash() {
      if (location.hash && !document.querySelector(':target')) {
        var element = document.getElementById('user-content-' + location.hash.slice(1));
        if (element) {
           element.scrollIntoView();
        }
      }
    }

    function autorefreshContent(eventSourceUrl) {
      var initialTitle = document.title;
      var contentElement = document.getElementById('grip-content');
      var source = new EventSource(eventSourceUrl);
      var isRendering = false;

      source.onmessage = function(ev) {
        var msg = JSON.parse(ev.data);
        if (msg.updating) {
          isRendering = true;
          document.title = '(Rendering) ' + document.title;
        } else {
          isRendering = false;
          document.title = initialTitle;
          contentElement.innerHTML = msg.content;
          showCanonicalImages();
        }
      }

      source.onerror = function(e) {
        if (e.readyState === EventSource.CLOSED && isRendering) {
          isRendering = false;
          document.title = initialTitle;
        }
      }
    }

    window.onhashchange = function() {
      scrollToHash();
    }

    window.onload = function() {
      scrollToHash();
    }

    showCanonicalImages();

    var autorefreshUrl = document.getElementById('preview-page').getAttribute('data-autorefresh-url');
    if (autorefreshUrl) {
      autorefreshContent(autorefreshUrl);
    }
  </script>
</body>
</html>
